import re
import site

import google.generativeai as genai
from faster_whisper import WhisperModel
from google.api_core import exceptions

import config
import time
import os


# åˆå§‹åŒ– Gemini
genai.configure(api_key=config.GEMINI_API_KEY)


def init_whisper_model():
    """
    ç»Ÿä¸€ä½¿ç”¨ CPU + int8 æ¨¡å¼ï¼Œå…¼é¡¾æœ¬åœ°ç¨³å®šæ€§å’Œ GitHub Actions
    """
    print("[*] æ­£åœ¨åŠ è½½ Whisper Medium æ¨¡å‹ (CPU ä¼˜åŒ–æ¨¡å¼)...")
    # 1. å±è”½ç¯å¢ƒå˜é‡ï¼Œé˜²æ­¢å®ƒå»æŠ“å–ç³»ç»Ÿä»£ç†
    if "http_proxy" in os.environ: del os.environ["http_proxy"]
    if "https_proxy" in os.environ: del os.environ["https_proxy"]

    # 2. å¼ºåˆ¶ HuggingFace ç¦»çº¿
    os.environ["HF_HUB_OFFLINE"] = "1"
    # int8 æ˜¯ CPU è¿è¡Œçš„é»„é‡‘é…ç½®ï¼Œå†…å­˜å ç”¨çº¦ 2.2GBï¼Œå‡†ç¡®ç‡æé«˜
    return WhisperModel("medium", device="cpu", compute_type="int8")


# åˆå§‹åŒ–
whisper_model = init_whisper_model()


def transcribe_audio(audio_path):
    try:
        print(f"  [*] å¼€å§‹æœ¬åœ°è½¬å†™ (å¼€å¯ VAD è¿‡æ»¤): {audio_path}")
        start_time = time.time()

        # --- å…³é”®å‚æ•°è°ƒæ•´ ---
        segments, info =whisper_model.transcribe(
            audio_path,
            beam_size=2,
            language="zh",
            vad_filter=True,
            condition_on_previous_text=False
        )

        full_text = ""
        for segment in segments:
            # è¿‡æ»¤æ‰è¿‡çŸ­ä¸”é‡å¤çš„æ— æ•ˆç‰‡æ®µ
            text = segment.text.strip()
            print(f"    [T] {segment.start:.1f}s -> {segment.text}")
            if len(text) > 1:  # å¿½ç•¥å•ä¸ªæ ‡ç‚¹æˆ–å•å­—
                full_text += text + " "

        # å¦‚æœæ„Ÿå¹å·ä¾ç„¶å¾ˆå¤šï¼Œæˆ‘ä»¬ä¸æŠ¥é”™ï¼Œç›´æ¥æ¸…æ´—æ‰å®ƒ
        if full_text.count('!') > 20 or full_text.count('ï¼') > 20:
            print("  [!] æ£€æµ‹åˆ°éƒ¨åˆ†å¹»å¬å†…å®¹ï¼Œæ­£åœ¨è¿›è¡Œæ¸…æ´—...")
            full_text = re.sub(r'[!ï¼]{2,}', ' ', full_text)

        duration = time.time() - start_time
        print(f"  [+] è½¬å†™å®Œæˆï¼Œè€—æ—¶: {duration:.2f}s")
        return full_text.strip()
    except Exception as e:
        print(f"  [X] æœ¬åœ°è½¬å†™å¤±è´¥: {e}")
        return None



def summarize_content(content_data):
    """
    åˆ†æ®µåˆ†æå…¨é‡å†…å®¹ï¼Œå½»åº•è§£å†³é•¿æ–‡æœ¬ 429 é—®é¢˜
    """
    model = genai.GenerativeModel('gemini-flash-latest')

    # 1. è·å–å¹¶æ¸…æ´—æ–‡æœ¬
    if content_data['type'] == 'text':
        full_text = content_data['content']
    else:
        full_text = transcribe_audio(content_data['path'])
        if not full_text: return "é”™è¯¯: è½¬å†™å¤±è´¥"

    # æ¸…æ´—æ‰ Whisper çš„å¹»å¬ä¹±ç 
    full_text = re.sub(r'([!ï¼?ï¼Ÿ\.ã€‚*])\1{2,}', r'\1', full_text)
    full_text = re.sub(r'\s+', ' ', full_text).strip()

    # 2. è®¾å®šåˆ†æ®µé€»è¾‘
    # 5000å­—ä¸€æ®µæ¯”è¾ƒä¿é™©ï¼Œæ—¢èƒ½ä¿ç•™ä¸Šä¸‹æ–‡ï¼Œåˆä¸å®¹æ˜“è§¦å‘ TPM é™åˆ¶
    CHUNK_SIZE = 5000
    chunks = [full_text[i:i + CHUNK_SIZE] for i in range(0, len(full_text), CHUNK_SIZE)]

    if len(chunks) == 1:
        # å¦‚æœå†…å®¹ä¸é•¿ï¼Œç›´æ¥èµ°å•æ¬¡æ€»ç»“
        return call_gemini_with_retry(model, full_text, "simple")

    # 3. åˆ†æ®µæå–æ ¸å¿ƒä¿¡æ¯ (Map é˜¶æ®µ)
    print(f"  [*] å†…å®¹è¿‡é•¿ï¼Œæ­£åœ¨åˆ† {len(chunks)} æ®µè¿›è¡Œæ·±åº¦åˆ†æ...")
    chunk_summaries = []

    for idx, chunk in enumerate(chunks):
        print(f"    - æ­£åœ¨åˆ†æç¬¬ {idx + 1}/{len(chunks)} æ®µ...")
        chunk_prompt = f"è¿™æ˜¯é•¿è§†é¢‘è½¬å½•ç¨¿çš„ç¬¬ {idx + 1} éƒ¨åˆ†ã€‚è¯·æå–è¯¥éƒ¨åˆ†æ¶‰åŠçš„æ‰€æœ‰å¸ç§ã€ç‚¹ä½ã€è¡Œæƒ…åˆ¤æ–­å’Œæ ¸å¿ƒé€»è¾‘ã€‚ä¸éœ€è¦æ ¼å¼åŒ–ï¼Œè¯·åˆ—å‡ºè¦ç‚¹ï¼š"

        summary = call_gemini_with_retry(model, chunk, chunk_prompt)
        chunk_summaries.append(summary)

        # å…³é”®ï¼šæ¯æ®µä¹‹é—´å¼ºåˆ¶ä¼‘æ¯ï¼Œé˜²æ­¢è§¦å‘ 429
        # å¦‚æœä¾ç„¶æŠ¥ 429ï¼Œè¯·æŠŠè¿™ä¸ªæ—¶é—´è°ƒé•¿åˆ° 15-20
        time.sleep(30)

    # 4. èšåˆæœ€ç»ˆæŠ¥å‘Š (Reduce é˜¶æ®µ)
    print("  [*] æ­£åœ¨èšåˆæ‰€æœ‰åˆ†æ®µä¿¡æ¯ï¼Œç”Ÿæˆæœ€ç»ˆç®€æŠ¥...")
    final_input = "\n\n".join(chunk_summaries)
    final_prompt = """
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯ä¸å†…å®¹åˆ†æå¸ˆã€‚ä¸‹é¢æ˜¯åŒä¸€æ®µé•¿è§†é¢‘çš„å„éƒ¨åˆ†è¦ç‚¹æå–ã€‚
    è¯·å°†è¿™äº›ä¿¡æ¯æ•´åˆæˆä¸€ä»½é€»è¾‘ä¸¥å¯†çš„ä¸­æ–‡ç®€æŠ¥ã€‚

    è¾“å‡ºæ ¼å¼è¦æ±‚ (Markdown):
    ### ğŸ“ ä¸€å¥è¯æ€»ç»“
    (50å­—ä»¥å†…æ¦‚æ‹¬æ ¸å¿ƒ)

    ### ğŸ’¡ æ ¸å¿ƒè§‚ç‚¹
    * (åˆ—å‡ºæ‰€æœ‰å…³é”®ç‚¹ä½å’Œé€»è¾‘)

    ### ğŸ“– è¯¦ç»†å†…å®¹/æ•™ç¨‹
    (åˆ—å‡ºç»†èŠ‚)
    """

    return call_gemini_with_retry(model, final_input, final_prompt)


def call_gemini_with_retry(model, text, task_type):
    """
    å°è£…çš„é€šç”¨è°ƒç”¨å‡½æ•°ï¼Œå¸¦é‡è¯•é€»è¾‘
    """
    # åœ¨å‘èµ·ç½‘ç»œè¯·æ±‚å‰ï¼Œä¸´æ—¶è®¾ç½®æœ¬åœ°ä»£ç†
    if config.LOCAL_PROXY:
        os.environ["http_proxy"] = config.LOCAL_PROXY
        os.environ["https_proxy"] = config.LOCAL_PROXY

    if task_type == "simple":
        prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯ä¸å†…å®¹åˆ†æå¸ˆã€‚è¯·æ ¹æ®è¾“å…¥çš„å†…å®¹ï¼Œç”¨ä¸­æ–‡è¾“å‡ºä¸€ä»½ç®€æŠ¥ï¼ˆä¸€å¥è¯æ€»ç»“ã€æ ¸å¿ƒè§‚ç‚¹ã€è¯¦ç»†ç»†èŠ‚ï¼‰ã€‚å†…å®¹å¦‚ä¸‹ï¼š\n\n"
    else:
        prompt = task_type  # ä¼ å…¥è‡ªå®šä¹‰ prompt

    max_retries = 3
    for i in range(max_retries):
        try:
            response = model.generate_content(
                prompt + "\n\n" + text,
                request_options={"timeout": 120}
            )
            return response.text
        except exceptions.ResourceExhausted:
            wait_time = (i + 1) * 30
            print(f"      [!] è§¦å‘é™é¢ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
            time.sleep(wait_time)
        except Exception as e:
            return f"è°ƒç”¨å‡ºé”™: {str(e)}"
        finally:
            # ç»“æŸåæ¸…é™¤ç¯å¢ƒå˜é‡ï¼Œä¿æŒç¯å¢ƒçº¯å‡€
            if "http_proxy" in os.environ: del os.environ["http_proxy"]
            if "https_proxy" in os.environ: del os.environ["https_proxy"]
    return "å¤šæ¬¡å°è¯•å API ä¾ç„¶æ‹’ç»è¯·æ±‚ã€‚"


